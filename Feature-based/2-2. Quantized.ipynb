{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXjhBQaJy5Xw"
   },
   "source": [
    "# 초기 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQrBCUI-QeMv",
    "outputId": "dfa255f8-f3be-49f8-b44e-ce98e7af9c0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 20 20:10:59 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   77C    P0              34W /  70W |    187MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GdbKTtjvQgna",
    "outputId": "e33e9e5b-5673-49a2-dc0a-92a3747e0756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 54.8 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfSmMrWDHMwG",
    "outputId": "dd5f778d-5953-4a60-db53-73341871ae3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.12.1)\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.0)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.6)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Colab 셀에 입력하여 실행\n",
    "!pip install librosa h5py optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPs4rn5qy6wL"
   },
   "source": [
    "# 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXZHyXQPGki0",
    "outputId": "16f5fd4e-36b8-4b0d-d0d1-7d6a1939e8e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- 라이브러리 임포트 -----------------------------\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.cuda.amp  # 혼합 정밀도 학습을 위한 torch.cuda.amp 추가\n",
    "\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "# ----------------------------- 환경 설정 -----------------------------\n",
    "# Google Drive 마운트\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wJB2xHlzA8G"
   },
   "source": [
    "# 환경설정, 전처리 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ml8jgWcNzE94",
    "outputId": "f2b85f9f-35d9-4586-9881-4daf8f9ebf53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "사용 중인 디바이스: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- 환경 설정 -----------------------------\n",
    "# Google Drive 마운트\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 일반 설정\n",
    "BATCH_SIZE = 512  # 배치 크기\n",
    "EPOCHS = 200  # 에포크 수\n",
    "LEARNING_RATE = 1e-3  # 학습률\n",
    "\n",
    "# 데이터 관련 설정 (Google Drive 내 경로로 변경)\n",
    "DATA_DIR = \"/content/drive/MyDrive/Sonus/Feature-based\"  # 데이터 디렉토리\n",
    "METADATA_FILE = os.path.join(DATA_DIR, \"metadata.json\")  # 메타데이터 파일 경로\n",
    "HDF5_FILE = os.path.join(DATA_DIR, \"preprocessed_data.h5\")  # 전처리된 데이터 저장 경로\n",
    "N_MFCC = 40  # MFCC 계수의 수\n",
    "MAX_LEN = 174  # MFCC 벡터의 최대 길이\n",
    "\n",
    "# 모델 및 로그 디렉토리 설정 (Google Drive 내 경로로 변경)\n",
    "MODELS_DIR = os.path.join(DATA_DIR, \"models\")  # 모델 저장 디렉토리\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "LOGS_DIR = os.path.join(DATA_DIR, \"logs\")  # 로그 디렉토리\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "# 하이퍼파라미터 튜닝 설정\n",
    "APPLY_HYPERPARAMETER_TUNING = False  # 하이퍼파라미터 튜닝 적용 여부\n",
    "\n",
    "# ----------------------------- GPU 및 CPU 설정 -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 디바이스: {device}\\n\")\n",
    "\n",
    "# CPU 사용 제한 설정 (필요에 따라 조정 가능)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"4\"\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noD1AWmfzQtr"
   },
   "source": [
    "# 함수 모음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKDCylduzIzW"
   },
   "outputs": [],
   "source": [
    "# ----------------------------- 전처리 함수 정의 -----------------------------\n",
    "def sanitize_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    문자열에서 알파벳, 숫자, 언더스코어만 남기고 나머지는 제거합니다.\n",
    "\n",
    "    Args:\n",
    "        name (str): 원본 문자열.\n",
    "\n",
    "    Returns:\n",
    "        str: 정제된 문자열.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[^a-zA-Z0-9_]\", \"\", name.replace(\" \", \"_\"))\n",
    "\n",
    "def load_metadata(metadata_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    메타데이터를 로드합니다.\n",
    "\n",
    "    Args:\n",
    "        metadata_path (str): 메타데이터 파일의 경로.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: 메타데이터 리스트.\n",
    "    \"\"\"\n",
    "    metadata = []\n",
    "    with open(metadata_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            metadata.append(json.loads(line))\n",
    "    return metadata\n",
    "\n",
    "def extract_mfcc(file_path: str, n_mfcc: int = N_MFCC, max_len: int = MAX_LEN) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    오디오 파일에서 MFCC 특징을 추출합니다.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): 오디오 파일의 경로.\n",
    "        n_mfcc (int): 추출할 MFCC 계수의 수.\n",
    "        max_len (int): MFCC 벡터의 최대 길이.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 고정된 크기의 MFCC 배열.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None, mono=True)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        if mfcc.shape[1] < max_len:\n",
    "            pad_width = max_len - mfcc.shape[1]\n",
    "            mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode=\"constant\")\n",
    "        else:\n",
    "            mfcc = mfcc[:, :max_len]\n",
    "        return mfcc\n",
    "    except Exception as e:\n",
    "        print(f\"MFCC 추출 실패: {file_path}, 에러: {e}\")\n",
    "        return np.zeros((n_mfcc, max_len))\n",
    "\n",
    "def preprocess_and_save_data(\n",
    "    metadata: List[Dict[str, Any]], data_dir: str, hdf5_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    데이터 전처리 및 HDF5 파일로 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        metadata (List[Dict[str, Any]]): 메타데이터 리스트.\n",
    "        data_dir (str): 오디오 데이터가 저장된 디렉토리.\n",
    "        hdf5_path (str): 저장할 HDF5 파일의 경로.\n",
    "    \"\"\"\n",
    "    total_samples = len(metadata)\n",
    "    with h5py.File(hdf5_path, \"w\") as h5f:\n",
    "        X_ds = h5f.create_dataset(\n",
    "            \"X\", shape=(total_samples, N_MFCC, MAX_LEN), dtype=np.float32\n",
    "        )\n",
    "        Y_list = []\n",
    "        for idx, data in enumerate(tqdm(metadata, desc=\"전처리 중\")):\n",
    "            sample_path = os.path.join(\n",
    "                data_dir, data[\"relative_path\"], data[\"sample_name\"]\n",
    "            )\n",
    "            mfcc = extract_mfcc(sample_path)\n",
    "            X_ds[idx] = mfcc\n",
    "            instruments = data[\"instruments\"]\n",
    "            Y_list.append(instruments)\n",
    "        Y_encoded_strings = [json.dumps(instr_list) for instr_list in Y_list]\n",
    "        dt = h5py.string_dtype(encoding=\"utf-8\")\n",
    "        h5f.create_dataset(\n",
    "            \"Y\", data=np.array(Y_encoded_strings, dtype=object), dtype=dt\n",
    "        )\n",
    "\n",
    "def load_preprocessed_data(hdf5_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    전처리된 데이터를 HDF5 파일에서 로드합니다.\n",
    "\n",
    "    Args:\n",
    "        hdf5_path (str): HDF5 파일의 경로.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: 특징 배열 X와 레이블 리스트 Y.\n",
    "    \"\"\"\n",
    "    with h5py.File(hdf5_path, \"r\") as h5f:\n",
    "        X = h5f[\"X\"][:]\n",
    "        Y = h5f[\"Y\"][:]\n",
    "    return X, Y\n",
    "\n",
    "def encode_labels(Y: np.ndarray) -> Tuple[np.ndarray, MultiLabelBinarizer, List[str]]:\n",
    "    \"\"\"\n",
    "    레이블을 이진 벡터로 인코딩합니다.\n",
    "\n",
    "    Args:\n",
    "        Y (np.ndarray): 레이블 리스트 (JSON-encoded strings).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, MultiLabelBinarizer, List[str]]: 인코딩된 레이블 배열, 레이블 변환기, 모든 악기 리스트.\n",
    "    \"\"\"\n",
    "    all_instruments = set()\n",
    "    parsed_Y = []\n",
    "    for instruments in Y:\n",
    "        if isinstance(instruments, bytes):\n",
    "            instruments = instruments.decode(\"utf-8\")\n",
    "        if isinstance(instruments, str):\n",
    "            try:\n",
    "                instruments = json.loads(instruments)\n",
    "            except json.JSONDecodeError:\n",
    "                instruments = instruments.split(\",\")\n",
    "        if not isinstance(instruments, list):\n",
    "            instruments = []\n",
    "        instruments = [instr.strip() for instr in instruments if instr.strip()]\n",
    "        all_instruments.update(instruments)\n",
    "        parsed_Y.append(instruments)\n",
    "    all_instruments = sorted(list(all_instruments))\n",
    "\n",
    "    if not all_instruments:\n",
    "        print(\"경고: 모든 악기 리스트가 비어 있습니다.\")\n",
    "\n",
    "    mlb = MultiLabelBinarizer(classes=all_instruments)\n",
    "    Y_encoded = mlb.fit_transform(parsed_Y)\n",
    "    return Y_encoded, mlb, all_instruments\n",
    "\n",
    "# ----------------------------- 데이터셋 및 데이터로더 클래스 정의 -----------------------------\n",
    "class MFCCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    MFCC 데이터를 위한 PyTorch Dataset 클래스\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray, augment: bool = False):\n",
    "        \"\"\"\n",
    "        초기화 함수\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): 특징 데이터.\n",
    "            Y (np.ndarray): 레이블 데이터.\n",
    "            augment (bool): 데이터 증강 여부.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        데이터셋의 길이를 반환합니다.\n",
    "\n",
    "        Returns:\n",
    "            int: 데이터셋의 길이.\n",
    "        \"\"\"\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.X[idx]\n",
    "        y = self.Y[idx]\n",
    "\n",
    "        # 데이터 전처리: 차원 변경 및 채널 추가\n",
    "        x = np.expand_dims(x, axis=0)  # (1, height, width)\n",
    "        x = torch.from_numpy(x).float()\n",
    "\n",
    "        if self.augment:\n",
    "            x = augment_batch(x)\n",
    "\n",
    "        mean = x.mean()\n",
    "        std = x.std()\n",
    "        x = (x - mean) / (std + 1e-6)\n",
    "\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def augment_batch(batch_X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    데이터 증강을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        batch_X (torch.Tensor): 배치의 MFCC 데이터. (batch_size, height, width)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 증강된 배치의 MFCC 데이터.\n",
    "    \"\"\"\n",
    "    # 랜덤 스케일링\n",
    "    scale = random.uniform(0.8, 1.2)\n",
    "    batch_X = batch_X * scale\n",
    "        \n",
    "    # 잡음 추가\n",
    "    noise = torch.randn_like(batch_X) * 0.05\n",
    "    batch_X = batch_X + noise\n",
    "\n",
    "    # 주파수 마스킹\n",
    "    freq_masking = random.randint(0, batch_X.shape[2] // 10)  # width\n",
    "    if freq_masking > 0:\n",
    "        f0 = random.randint(0, batch_X.shape[2] - freq_masking - 1)\n",
    "        batch_X[:, :, f0 : f0 + freq_masking] = 0\n",
    "\n",
    "    # 시간 마스킹\n",
    "    time_masking = random.randint(0, batch_X.shape[1] // 10)  # height\n",
    "    if time_masking > 0:\n",
    "        t0 = random.randint(0, batch_X.shape[1] - time_masking - 1)\n",
    "        batch_X[:, t0 : t0 + time_masking, :] = 0\n",
    "\n",
    "    return batch_X\n",
    "\n",
    "\n",
    "def create_data_loaders(\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    데이터로더를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        X_train, Y_train, X_val, Y_val, X_test, Y_test: 학습, 검증, 테스트 데이터와 레이블\n",
    "\n",
    "    Returns:\n",
    "        Tuple[DataLoader, DataLoader, DataLoader]: 학습, 검증, 테스트 데이터로더\n",
    "    \"\"\"\n",
    "    train_dataset = MFCCDataset(X_train, Y_train, augment=True)\n",
    "    val_dataset = MFCCDataset(X_val, Y_val, augment=False)\n",
    "    test_dataset = MFCCDataset(X_test, Y_test, augment=False)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ----------------------------- 모델 정의 -----------------------------\n",
    "def calculate_padding(kernel_size: int, stride: int, input_size: int) -> int:\n",
    "    \"\"\"\n",
    "    패딩 크기를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "        kernel_size (int): 커널 크기.\n",
    "        stride (int): 스트라이드 크기.\n",
    "        input_size (int): 입력 크기.\n",
    "\n",
    "    Returns:\n",
    "        int: 패딩 크기.\n",
    "    \"\"\"\n",
    "    return (kernel_size - stride) // 2\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN 모델 정의 클래스\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int, int, int],\n",
    "        num_classes: int,\n",
    "        dropout_rate: float = 0.5,\n",
    "        l2_reg: float = 0.001,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        모델을 초기화합니다.\n",
    "\n",
    "        Args:\n",
    "            input_shape (Tuple[int, int, int]): 입력 데이터의 형태.\n",
    "            num_classes (int): 출력 클래스의 수.\n",
    "            dropout_rate (float): 드롭아웃 비율.\n",
    "            l2_reg (float): L2 정규화 계수.\n",
    "        \"\"\"\n",
    "        super(CNNModel, self).__init__()\n",
    "        _, input_height, input_width = input_shape\n",
    "\n",
    "        # 패딩 계산\n",
    "        self.padding1 = calculate_padding(3, 1, input_height)\n",
    "        self.padding2 = calculate_padding(3, 1, input_width)\n",
    "\n",
    "        self.quant = torch.quantization.QuantStub()  # QuantStub 추가\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), padding=(self.padding1, self.padding2))\n",
    "        self.pool1 = nn.MaxPool2d((2, 2))\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(self.padding1, self.padding2))\n",
    "        self.pool2 = nn.MaxPool2d((2, 2))\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=(self.padding1, self.padding2))\n",
    "        self.pool3 = nn.MaxPool2d((2, 2))\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        conv_output_size = self._get_conv_output((1, input_height, input_width))\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(conv_output_size, 128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.dequant = torch.quantization.DeQuantStub()  # DeQuantStub 추가\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "    def _get_conv_output(self, shape: Tuple[int, int, int]) -> int:\n",
    "        \"\"\"\n",
    "        합성곱 계층의 출력을 계산합니다.\n",
    "\n",
    "        Args:\n",
    "            shape (Tuple[int, int, int]): 입력 데이터의 형태.\n",
    "\n",
    "        Returns:\n",
    "            int: 합성곱 계층 출력의 크기.\n",
    "        \"\"\"\n",
    "        bs = 1\n",
    "        input = torch.zeros(bs, *shape)\n",
    "        output_feat = self._forward_features(input)\n",
    "        n_size = output_feat.data.view(bs, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def _forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        합성곱 계층을 통과하는 부분입니다.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 입력 텐서.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 합성곱 계층의 출력.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.bn3(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        모델의 순전파를 정의합니다.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 입력 텐서.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 모델의 출력.\n",
    "        \"\"\"\n",
    "        x = self.quant(x)  # QuantStub로 양자화 적용\n",
    "        x = self._forward_features(x)\n",
    "        x = x.reshape(x.size(0), -1)  # Flatten\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.output_layer(x)  # Sigmoid 제거 (BCEWithLogitsLoss 사용)\n",
    "        x = self.dequant(x)  # DeQuantStub로 역양자화 적용\n",
    "        return x \n",
    "\n",
    "# ----------------------------- 모델 학습 및 평가 함수 정의 -----------------------------\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs: int,\n",
    "    device: torch.device,\n",
    "):\n",
    "    \"\"\"\n",
    "    모델을 훈련합니다.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): 학습할 모델.\n",
    "        train_loader (DataLoader): 훈련 데이터 로더.\n",
    "        val_loader (DataLoader): 검증 데이터 로더.\n",
    "        criterion: 손실 함수.\n",
    "        optimizer: 옵티마이저.\n",
    "        scheduler: 학습률 스케줄러.\n",
    "        num_epochs (int): 에포크 수.\n",
    "        device (torch.device): 장치 (CPU 또는 GPU).\n",
    "    \"\"\"\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 10\n",
    "    trigger_times = 0\n",
    "\n",
    "    # 혼합 정밀도 스케일러 초기화\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    print(\"모델 학습 시작...\\n\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        # 훈련 단계\n",
    "        train_bar = tqdm(\n",
    "            train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] 훈련 중\", leave=False\n",
    "        )\n",
    "        for batch_X, batch_Y in train_bar:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_Y = batch_Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):  # 혼합 정밀도 적용\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_Y)\n",
    "                # L2 정규화는 옵티마이저의 weight_decay로 처리하므로 추가하지 않음\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * batch_X.size(0)\n",
    "            total_samples += batch_X.size(0)\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_correct += (preds == batch_Y).sum().item()\n",
    "\n",
    "            # 배치별 손실 업데이트\n",
    "            train_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = total_correct / (total_samples * batch_Y.size(1))\n",
    "\n",
    "        # 검증 단계\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_samples = 0\n",
    "        val_correct = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(\n",
    "                val_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] 검증 중\", leave=False\n",
    "            )\n",
    "            for batch_X, batch_Y in val_bar:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_Y = batch_Y.to(device)\n",
    "\n",
    "                with torch.amp.autocast(\"cuda\"):  # 혼합 정밀도 적용\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_Y)\n",
    "                    # L2 정규화는 옵티마이저의 weight_decay로 처리하므로 추가하지 않음\n",
    "\n",
    "                val_loss += loss.item() * batch_X.size(0)\n",
    "                val_samples += batch_X.size(0)\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_correct += (preds == batch_Y).sum().item()\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(batch_Y.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / val_samples\n",
    "        val_acc = val_correct / (val_samples * batch_Y.size(1))\n",
    "\n",
    "        # F1-score, Precision, Recall 계산\n",
    "        if len(np.unique(all_labels)) > 1:\n",
    "            epoch_f1 = f1_score(all_labels, all_preds, average=\"samples\", zero_division=0)\n",
    "            epoch_precision = precision_score(\n",
    "                all_labels, all_preds, average=\"samples\", zero_division=0\n",
    "            )\n",
    "            epoch_recall = recall_score(\n",
    "                all_labels, all_preds, average=\"samples\", zero_division=0\n",
    "            )\n",
    "        else:\n",
    "            epoch_f1 = 0.0\n",
    "            epoch_precision = 0.0\n",
    "            epoch_recall = 0.0\n",
    "\n",
    "        # 에포크별 성능 지표 출력\n",
    "        tqdm.write(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] 완료 - \"\n",
    "            f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "            f\"Val Precision: {epoch_precision:.4f}, Val Recall: {epoch_recall:.4f}, Val F1-Score: {epoch_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 학습률 스케줄러 단계\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early Stopping 체크\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            trigger_times = 0\n",
    "            save_model(\n",
    "                model,\n",
    "                os.path.join(MODELS_DIR, \"best_model.pt\"),\n",
    "            )\n",
    "            tqdm.write(f\"--> 최고 검증 손실 기록: {best_val_loss:.4f} 저장됨.\\n\")\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            tqdm.write(f\"--> EarlyStopping 트리거 횟수: {trigger_times}/{patience}\\n\")\n",
    "            if trigger_times >= patience:\n",
    "                tqdm.write(\"Early stopping 발생. 학습 중단.\\n\")\n",
    "                break\n",
    "\n",
    "    tqdm.write(f\"모델 학습 완료. 최고 검증 손실: {best_val_loss:.4f}\\n\")\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module, test_loader: DataLoader, criterion, device: torch.device\n",
    ") -> Tuple[float, float, List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    모델을 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): 평가할 모델.\n",
    "        test_loader (DataLoader): 테스트 데이터 로더.\n",
    "        criterion: 손실 함수.\n",
    "        device (torch.device): 장치 (CPU 또는 GPU).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, List[float], List[float]]: 테스트 손실, 정확도, 예측 값 리스트, 실제 값 리스트.\n",
    "    \"\"\"\n",
    "    # 모델과 데이터를 CPU로 이동\n",
    "    model.to(device)  # 모델을 CPU로 이동\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    total_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # 가중치도 cpu로 이동\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weight_tensor.to(device))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in test_loader:\n",
    "            # 배치를 명시적으로 CPU로 이동\n",
    "            batch_X = batch_X.to('cpu').float()  # batch_X를 CPU로\n",
    "            batch_Y = batch_Y.to('cpu').float()  # batch_Y를 CPU로\n",
    "\n",
    "            # 모델 예측 수행\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_Y)  # 손실 계산\n",
    "\n",
    "            test_loss += loss.item() * batch_X.size(0)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            # 결과를 numpy 배열로 변환 (CPU 상에서)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_Y.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    avg_test_acc = (\n",
    "        (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    )  # 정확도 계산\n",
    "\n",
    "    return avg_test_loss, avg_test_acc, all_preds, all_labels\n",
    "\n",
    "\n",
    "def save_model(model: nn.Module, path: str) -> None:\n",
    "    \"\"\"\n",
    "    모델을 지정된 경로에 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): 저장할 모델.\n",
    "        path (str): 저장할 파일 경로.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"모델 저장 완료: {path}\")\n",
    "\n",
    "def load_model(model: nn.Module, path: str, device: torch.device) -> nn.Module:\n",
    "    \"\"\"\n",
    "    모델을 지정된 경로에서 로드합니다.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): 로드할 모델 구조.\n",
    "        path (str): 모델 파일 경로.\n",
    "        device (torch.device): 장치 (CPU 또는 GPU).\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: 로드된 모델.\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_quantized_model(model_path: str, input_shape: Tuple[int, int, int], num_classes: int, device: torch.device) -> nn.Module:\n",
    "    \"\"\"\n",
    "    양자화된 모델을 불러옵니다.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): 저장된 양자화된 모델 파일 경로.\n",
    "        input_shape (Tuple[int, int, int]): 모델의 입력 데이터 형태.\n",
    "        num_classes (int): 클래스 수.\n",
    "        device (torch.device): 사용할 디바이스 (CPU 또는 GPU).\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: 불러온 양자화된 모델.\n",
    "    \"\"\"\n",
    "    # x86 엔진 설정\n",
    "    torch.backends.quantized.engine = 'x86'\n",
    "\n",
    "    # 양자화된 모델 구조 정의\n",
    "    model = CNNModel(input_shape=input_shape, num_classes=num_classes)\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('x86')\n",
    "\n",
    "    # 양자화 준비 및 변환\n",
    "    torch.quantization.prepare(model, inplace=True)\n",
    "    torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "    # 저장된 양자화된 모델 가중치 로드\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    # 모델을 지정된 디바이스로 이동\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "# ----------------------------- 개별 악기 예측 함수 정의 -----------------------------\n",
    "def predict_instruments(audio_file_path: str, model_path: str, mlb: MultiLabelBinarizer) -> List[str]:\n",
    "    # MFCC 추출\n",
    "    mfcc = extract_mfcc(audio_file_path)\n",
    "    if mfcc.shape != (N_MFCC, MAX_LEN):\n",
    "        raise ValueError(f\"MFCC 크기가 예상 범위를 벗어남: {mfcc.shape}\")\n",
    "\n",
    "    # 배치 및 채널 차원 추가\n",
    "    mfcc = np.expand_dims(mfcc, axis=0)  # (1, N_MFCC, MAX_LEN)\n",
    "    mfcc = np.expand_dims(mfcc, axis=0)  # (1, 1, N_MFCC, MAX_LEN)\n",
    "\n",
    "    # 텐서 변환\n",
    "    mfcc_tensor = torch.tensor(mfcc, dtype=torch.float32)\n",
    "\n",
    "    # 정규화\n",
    "    mean = mfcc_tensor.mean()\n",
    "    std = mfcc_tensor.std()\n",
    "    mfcc_tensor = (mfcc_tensor - mean) / (std + 1e-6)\n",
    "\n",
    "    # 모델 로드\n",
    "    device_cpu = torch.device('cpu')\n",
    "    input_shape = (1, N_MFCC, MAX_LEN)\n",
    "    num_classes = len(mlb.classes_)\n",
    "\n",
    "    # 양자화된 모델 정의\n",
    "    model = CNNModel(input_shape, num_classes)\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('x86')  # 양자화 구성 동일하게 설정\n",
    "    torch.backends.quantized.engine = 'x86'\n",
    "    torch.quantization.prepare(model, inplace=True)\n",
    "    torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "    # 양자화된 모델 가중치 로드\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device_cpu))\n",
    "    model.to(device_cpu)\n",
    "    model.eval()\n",
    "\n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        output = model(mfcc_tensor)\n",
    "        sigmoid_output = torch.sigmoid(output).cpu().numpy()[0]\n",
    "        print(\"Sigmoid Outputs:\", sigmoid_output)\n",
    "\n",
    "        # 클래스별 확률 값 리스트 생성\n",
    "        class_probabilities = [f\"{mlb.classes_[i]}: {round(prob, 4)}\" for i, prob in enumerate(sigmoid_output)]\n",
    "\n",
    "        # 한 줄 리스트 형태로 출력\n",
    "        print(\"0 ~ 1 Outputs:\", \", \".join(class_probabilities))\n",
    "\n",
    "        predicted = sigmoid_output > 0.5\n",
    "        predicted_instruments = [mlb.classes_[i] for i, val in enumerate(predicted) if val]\n",
    "\n",
    "    return predicted_instruments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mxXZHJxzjge"
   },
   "source": [
    "# 전처리 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQhWFujQzjGx",
    "outputId": "a4e66221-3d43-40e3-cd7b-6a5a93737bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메타데이터 로드 중...\n",
      "메타데이터 로드 완료: 231980 샘플\n",
      "전처리된 데이터 파일이 존재합니다. 로드합니다.\n",
      "전처리된 데이터 로드 중...\n",
      "전처리된 데이터 로드 완료: X shape=(231980, 40, 174), Y shape=(231980,)\n",
      "레이블 인코딩 중...\n",
      "레이블 인코딩 완료: 16 종류의 악기\n",
      "데이터셋 분할 중...\n",
      "데이터셋 분할 완료:\n",
      " - 훈련 세트: 185584 샘플\n",
      " - 검증 세트: 23198 샘플\n",
      " - 테스트 세트: 23198 샘플\n"
     ]
    }
   ],
   "source": [
    "# 메타데이터 로드\n",
    "print(\"메타데이터 로드 중...\")\n",
    "metadata = load_metadata(METADATA_FILE)\n",
    "print(f\"메타데이터 로드 완료: {len(metadata)} 샘플\")\n",
    "\n",
    "# 데이터 전처리 및 저장\n",
    "if not os.path.exists(HDF5_FILE):\n",
    "    print(\"데이터 전처리 시작...\")\n",
    "    preprocess_and_save_data(metadata, DATA_DIR, HDF5_FILE)\n",
    "    print(\"데이터 전처리 및 저장 완료.\")\n",
    "else:\n",
    "    print(\"전처리된 데이터 파일이 존재합니다. 로드합니다.\")\n",
    "\n",
    "# 전처리된 데이터 로드\n",
    "print(\"전처리된 데이터 로드 중...\")\n",
    "X, Y = load_preprocessed_data(HDF5_FILE)\n",
    "print(f\"전처리된 데이터 로드 완료: X shape={X.shape}, Y shape={Y.shape}\")\n",
    "\n",
    "# 레이블 인코딩\n",
    "print(\"레이블 인코딩 중...\")\n",
    "Y_encoded, mlb, all_instruments = encode_labels(Y)\n",
    "print(f\"레이블 인코딩 완료: {len(all_instruments)} 종류의 악기\")\n",
    "\n",
    "if len(all_instruments) == 0:\n",
    "    raise ValueError(\"레이블 인코딩 실패: 악기 리스트가 비어 있습니다.\")\n",
    "\n",
    "# 데이터셋 분할\n",
    "print(\"데이터셋 분할 중...\")\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(\n",
    "    X, Y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(\n",
    "    X_temp, Y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "print(f\"데이터셋 분할 완료:\")\n",
    "print(f\" - 훈련 세트: {X_train.shape[0]} 샘플\")\n",
    "print(f\" - 검증 세트: {X_val.shape[0]} 샘플\")\n",
    "print(f\" - 테스트 세트: {X_test.shape[0]} 샘플\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beOzYDkIzu9O"
   },
   "source": [
    "# 모델 학습 / 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZVL3vkkzwm-",
    "outputId": "0c904b30-6893-4b92-cc9f-c450a24ca467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 가중치 계산 중...\n",
      "클래스 가중치: tensor([0.9408, 0.9405, 0.9382, 0.9408, 0.9441, 0.9405, 0.9424, 0.9412, 0.9411,\n",
      "        0.9395, 0.9421, 0.9443, 0.9401, 0.9431, 0.9435, 0.9429],\n",
      "       device='cuda:0')\n",
      "테스트 데이터 평가 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:1315: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "<ipython-input-48-8e89d2801f77>:623: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 0.4078852299888932\n",
      "테스트 정확도: 0.8158596646262609\n",
      "F1-score: 0.7290878684355311\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "bass clarinet       0.87      0.79      0.83     12396\n",
      "      bassoon       0.90      0.84      0.87     12471\n",
      "        cello       0.77      0.71      0.74     12391\n",
      "     clarinet       0.82      0.85      0.83     12483\n",
      "clash cymbals       1.00      1.00      1.00     12448\n",
      "  double bass       0.72      0.96      0.82     12534\n",
      "        flute       0.79      0.67      0.72     12412\n",
      "  french horn       0.79      0.71      0.75     12567\n",
      "         oboe       0.87      0.72      0.79     12488\n",
      "    saxophone       0.85      0.81      0.83     12395\n",
      "   tambourine       0.97      0.99      0.98     12391\n",
      "     trombone       0.82      0.81      0.81     12381\n",
      "      trumpet       0.83      0.80      0.81     12460\n",
      "         tuba       0.88      0.93      0.91     12383\n",
      "        viola       0.77      0.72      0.75     12371\n",
      "       violin       0.77      0.71      0.74     12461\n",
      "\n",
      "    micro avg       0.84      0.81      0.83    199032\n",
      "    macro avg       0.84      0.81      0.82    199032\n",
      " weighted avg       0.84      0.81      0.82    199032\n",
      "  samples avg       0.82      0.72      0.73    199032\n",
      "\n",
      "\n",
      "모든 작업 완료.\n",
      "개별 오디오 파일에 대한 악기 예측 중...\n",
      "Sigmoid Outputs: [0.5        0.9974268  0.729747   0.8793913  1.         0.99904543\n",
      " 0.729747   0.729747   0.5        0.729747   0.0483368  0.729747\n",
      " 0.729747   0.5        0.729747   0.729747  ]\n",
      "0 ~ 1 Outputs: bass clarinet: 0.5, bassoon: 0.9973999857902527, cello: 0.7297000288963318, clarinet: 0.8794000148773193, clash cymbals: 1.0, double bass: 0.9990000128746033, flute: 0.7297000288963318, french horn: 0.7297000288963318, oboe: 0.5, saxophone: 0.7297000288963318, tambourine: 0.04830000177025795, trombone: 0.7297000288963318, trumpet: 0.7297000288963318, tuba: 0.5, viola: 0.7297000288963318, violin: 0.7297000288963318\n",
      "예측된 악기: ['bassoon', 'cello', 'clarinet', 'clash cymbals', 'double bass', 'flute', 'french horn', 'saxophone', 'trombone', 'trumpet', 'viola', 'violin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:1315: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "<ipython-input-48-8e89d2801f77>:663: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device_cpu))\n"
     ]
    }
   ],
   "source": [
    "# 클래스 가중치 계산\n",
    "print(\"클래스 가중치 계산 중...\")\n",
    "# 멀티레이블 분류이므로 각 클래스에 대해 가중치를 계산\n",
    "class_weights = []\n",
    "for i in range(Y_encoded.shape[1]):\n",
    "    classes = np.unique(Y_train[:, i])\n",
    "    if len(classes) > 1:\n",
    "        cw = compute_class_weight(\n",
    "            class_weight=\"balanced\", classes=classes, y=Y_train[:, i]\n",
    "        )\n",
    "        class_weights.append(cw[1])  # Positive class weight\n",
    "    else:\n",
    "        class_weights.append(1.0)  # Default weight\n",
    "\n",
    "class_weight_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(f\"클래스 가중치: {class_weight_tensor}\")\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "if APPLY_HYPERPARAMETER_TUNING:\n",
    "    print(\"하이퍼파라미터 튜닝을 수행합니다...\")\n",
    "    # 일단 생략.............\n",
    "else:\n",
    "    learning_rate = LEARNING_RATE\n",
    "    dropout_rate = 0.5\n",
    "    num_epochs = EPOCHS\n",
    "\n",
    "# 모델 구축\n",
    "input_shape = (1, MAX_LEN, N_MFCC)  # 입력 형태 수정\n",
    "num_classes = Y_encoded.shape[1]  # 멀티레이블 분류를 위한 클래스 수\n",
    "model = CNNModel(input_shape, num_classes, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "# 옵티마이저와 손실 함수 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)  # L2 정규화 포함\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weight_tensor)\n",
    "\n",
    "# 학습률 스케줄러 설정\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
    ")\n",
    "\n",
    "# # 모델 훈련\n",
    "# print(\"모델 훈련 시작...\")\n",
    "# train_model(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     criterion=criterion,\n",
    "#     optimizer=optimizer,\n",
    "#     scheduler=scheduler,\n",
    "#     num_epochs=num_epochs,\n",
    "#     device=device,\n",
    "# )\n",
    "# print(\"모델 훈련 완료.\")\n",
    "\n",
    "# model = load_model(model,os.path.join(MODELS_DIR, f\"optimized_model.pt\"),device)\n",
    "\n",
    "# # 모델 양자화 적용 (Quantization)\n",
    "# print(\"모델 양자화 시작...\")\n",
    "# model.eval()\n",
    "# model.cpu()\n",
    "# model.qconfig = torch.quantization.get_default_qconfig('x86')\n",
    "# torch.backends.quantized.engine = 'x86'\n",
    "# torch.quantization.prepare(model, inplace=True)\n",
    "# # 캘리브레이션 (간단히 검증 데이터로 수행)\n",
    "# with torch.no_grad():\n",
    "#     for batch_X, _ in tqdm(val_loader, desc=\"양자화 캘리브레이션 중\", leave=False):\n",
    "#         batch_X = batch_X.to('cpu')\n",
    "#         outputs = model(batch_X)\n",
    "# torch.quantization.convert(model, inplace=True)\n",
    "# print(\"모델 양자화 완료.\")\n",
    "\n",
    "# 모델 저장\n",
    "# save_model(model, os.path.join(MODELS_DIR, \"optimized_model.pt\"))\n",
    "\n",
    "\n",
    "# CPU에서 평가\n",
    "device_cpu = torch.device(\"cpu\")  # CPU 장치 설정\n",
    "model.to(device_cpu)  # 모델을 CPU로 이동\n",
    "\n",
    "# 이미 학습된 모델 로드하기 위해..\n",
    "model = load_quantized_model(\n",
    "    model_path=os.path.join(MODELS_DIR, \"optimized_model.pt\"),\n",
    "    input_shape=input_shape,\n",
    "    num_classes=num_classes,\n",
    "    device=device_cpu\n",
    ")\n",
    "\n",
    "\n",
    "# 테스트 데이터 평가\n",
    "print(\"테스트 데이터 평가 중...\")\n",
    "avg_test_loss, avg_test_acc, all_preds, all_labels = evaluate_model(\n",
    "    model=model, test_loader=test_loader, criterion=criterion, device=device_cpu\n",
    ")\n",
    "print(f\"테스트 손실: {avg_test_loss}\")\n",
    "print(f\"테스트 정확도: {avg_test_acc}\")\n",
    "\n",
    "# F1-score 계산\n",
    "if len(np.unique(all_labels)) > 1:\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"samples\", zero_division=0)\n",
    "    print(f\"F1-score: {f1}\")\n",
    "else:\n",
    "    print(\"F1-score 계산 불가: 테스트 세트에 하나의 클래스만 존재합니다.\")\n",
    "\n",
    "# 분류 리포트 출력\n",
    "if len(np.unique(all_labels)) > 1:\n",
    "    report = classification_report(\n",
    "        all_labels,\n",
    "        all_preds,\n",
    "        target_names=mlb.classes_,\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(report)\n",
    "else:\n",
    "    print(\"분류 리포트 출력 불가: 테스트 세트에 하나의 클래스만 존재합니다.\")\n",
    "\n",
    "# 메모리 정리\n",
    "del model\n",
    "del train_loader\n",
    "del val_loader\n",
    "del test_loader\n",
    "gc.collect()\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n모든 작업 완료.\")\n",
    "\n",
    "# 예측할 오디오 파일 경로\n",
    "audio_file_path = \"/content/drive/MyDrive/Sonus/Feature-based/concerto_sample_1.mp3\" \n",
    "\n",
    "# 예측 수행\n",
    "print(\"개별 오디오 파일에 대한 악기 예측 중...\")\n",
    "predicted_instruments = predict_instruments(\n",
    "    audio_file_path,\n",
    "    os.path.join(MODELS_DIR, \"optimized_model.pt\"),\n",
    "    mlb\n",
    ")\n",
    "print(f\"예측된 악기: {predicted_instruments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6szPn60u0pLh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
